{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "933.65s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightly in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (1.5.14)\n",
      "Collecting mambavision\n",
      "  Using cached mambavision-1.0.9-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (4.48.3)\n",
      "Requirement already satisfied: mamba_ssm in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (2.2.4)\n",
      "Requirement already satisfied: timm in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (1.0.14)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2024.8.30)\n",
      "Requirement already satisfied: hydra-core>=1.0.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (1.3.2)\n",
      "Requirement already satisfied: lightly-utils~=0.0.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (0.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly) (2.9.0.post0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2.32.3)\n",
      "Requirement already satisfied: six>=1.10 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly) (1.16.0)\n",
      "Requirement already satisfied: tqdm>=4.44 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (4.67.0)\n",
      "Requirement already satisfied: torch in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly) (0.19.1)\n",
      "Requirement already satisfied: pydantic>=1.10.5 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2.9.2)\n",
      "Requirement already satisfied: pytorch-lightning>=1.0.4 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (1.9.5)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2.2.3)\n",
      "Requirement already satisfied: aenum>=3.1.11 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (3.1.15)\n",
      "Collecting timm\n",
      "  Using cached timm-0.9.0-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting mamba_ssm\n",
      "  Using cached mamba_ssm-1.0.1.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting causal-conv1d==1.0.2 (from mambavision)\n",
      "  Using cached causal_conv1d-1.0.2.tar.gz (6.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mambavision) (0.8.0)\n",
      "Requirement already satisfied: packaging in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (24.2)\n",
      "Requirement already satisfied: ninja in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (1.11.1.3)\n",
      "Requirement already satisfied: triton in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from mamba_ssm) (3.0.0)\n",
      "Requirement already satisfied: pyyaml in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (0.26.2)\n",
      "Requirement already satisfied: safetensors in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (0.4.5)\n",
      "Collecting buildtools (from causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached buildtools-1.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface-hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface-hub->timm) (4.12.2)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from hydra-core>=1.0.0->lightly) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from hydra-core>=1.0.0->lightly) (4.9.3)\n",
      "Requirement already satisfied: Pillow in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly-utils~=0.0.0->lightly) (10.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from pydantic>=1.10.5->lightly) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from pydantic>=1.10.5->lightly) (2.23.4)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from pytorch-lightning>=1.0.4->lightly) (1.6.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from pytorch-lightning>=1.0.4->lightly) (0.11.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests>=2.23.0->lightly) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests>=2.23.0->lightly) (3.10)\n",
      "Requirement already satisfied: sympy in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (1.13.2)\n",
      "Requirement already satisfied: networkx in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (3.3)\n",
      "Requirement already satisfied: jinja2 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->lightly) (12.6.68)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (3.11.7)\n",
      "Requirement already satisfied: setuptools in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightning-utilities>=0.6.0.post0->pytorch-lightning>=1.0.4->lightly) (75.1.0)\n",
      "Collecting sqlalchemy (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached SQLAlchemy-2.0.38-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting argparse (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting twisted (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting simplejson (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting furl (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached furl-2.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting docopt (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting redo (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached redo-3.0.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from jinja2->torch->lightly) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from sympy->torch->lightly) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.18.0)\n",
      "Collecting orderedmultidict>=1.0.1 (from furl->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting automat>=24.8.0 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting constantly>=15.1 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=24.7.0 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting zope-interface>=5 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Using cached mambavision-1.0.9-py3-none-any.whl (56 kB)\n",
      "Using cached timm-0.9.0-py3-none-any.whl (2.2 MB)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Using cached furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Using cached redo-3.0.0-py2.py3-none-any.whl (14 kB)\n",
      "Using cached simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Using cached SQLAlchemy-2.0.38-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
      "Using cached Automat-24.8.1-py3-none-any.whl (42 kB)\n",
      "Using cached constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
      "Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Using cached incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Using cached orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
      "Building wheels for collected packages: mamba_ssm, causal-conv1d\n",
      "  Building wheel for mamba_ssm (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[135 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.4.1+cu121\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m Guessing wheel URL:  https://github.com/state-spaces/mamba/releases/download/v1.0.1/mamba_ssm-1.0.1+cu122torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
      "  \u001b[31m   \u001b[0m Precompiled wheel not found. Building from source...\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/models\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/models/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/models\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/models/mixer_seq_simple.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/modules\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/modules/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/modules\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/modules/mamba_simple.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/ops\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/selective_scan_interface.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/utils\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/utils\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/utils/generation.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/utils\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/utils/hf.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/triton/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/triton/layernorm.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/triton/selective_state_update.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m /cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "  \u001b[31m   \u001b[0m /cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no g++ version bounds defined for CUDA version 12.5\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
      "  \u001b[31m   \u001b[0m building 'selective_scan_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-install-oxrjihye/mamba-ssm_265eed8a33904f6ca89b1fbe8803c998/build/temp.linux-x86_64-cpython-311/csrc/selective_scan\n",
      "  \u001b[31m   \u001b[0m Emitting ninja build file /tmp/pip-install-oxrjihye/mamba-ssm_265eed8a33904f6ca89b1fbe8803c998/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
      "  \u001b[31m   \u001b[0m Compiling objects...\n",
      "  \u001b[31m   \u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  \u001b[31m   \u001b[0m ninja: error: '/tmp/pip-install-oxrjihye/mamba-ssm_265eed8a33904f6ca89b1fbe8803c998/csrc/selective_scan/selective_scan.cpp', needed by '/tmp/pip-install-oxrjihye/mamba-ssm_265eed8a33904f6ca89b1fbe8803c998/build/temp.linux-x86_64-cpython-311/csrc/selective_scan/selective_scan.o', missing and no known rule to make it\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-oxrjihye/mamba-ssm_265eed8a33904f6ca89b1fbe8803c998/setup.py\", line 213, in run\n",
      "  \u001b[31m   \u001b[0m     urllib.request.urlretrieve(wheel_url, wheel_filename)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 241, in urlretrieve\n",
      "  \u001b[31m   \u001b[0m     with contextlib.closing(urlopen(url, data)) as fp:\n",
      "  \u001b[31m   \u001b[0m                             ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 216, in urlopen\n",
      "  \u001b[31m   \u001b[0m     return opener.open(url, data, timeout)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 525, in open\n",
      "  \u001b[31m   \u001b[0m     response = meth(req, response)\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 634, in http_response\n",
      "  \u001b[31m   \u001b[0m     response = self.parent.error(\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 563, in error\n",
      "  \u001b[31m   \u001b[0m     return self._call_chain(*args)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 496, in _call_chain\n",
      "  \u001b[31m   \u001b[0m     result = func(*args)\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 643, in http_error_default\n",
      "  \u001b[31m   \u001b[0m     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "  \u001b[31m   \u001b[0m urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m During handling of the above exception, another exception occurred:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2105, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     subprocess.run(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/subprocess.py\", line 571, in run\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-oxrjihye/mamba-ssm_265eed8a33904f6ca89b1fbe8803c998/setup.py\", line 233, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/__init__.py\", line 117, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 183, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-oxrjihye/mamba-ssm_265eed8a33904f6ca89b1fbe8803c998/setup.py\", line 230, in run\n",
      "  \u001b[31m   \u001b[0m     super().run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/wheel/_bdist_wheel.py\", line 378, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 98, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 866, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     build_ext.build_extensions(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 476, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 502, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 263, in build_extension\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_extension(self, ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 557, in build_extension\n",
      "  \u001b[31m   \u001b[0m     objects = self.compiler.compile(\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 679, in unix_wrap_ninja_compile\n",
      "  \u001b[31m   \u001b[0m     _write_ninja_file_and_compile_objects(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1785, in _write_ninja_file_and_compile_objects\n",
      "  \u001b[31m   \u001b[0m     _run_ninja_build(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2121, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(message) from e\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Error compiling objects for extension\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for mamba_ssm\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for mamba_ssm\n",
      "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[119 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.4.1+cu121\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m Guessing wheel URL:  https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.0.2/causal_conv1d-1.0.2+cu122torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
      "  \u001b[31m   \u001b[0m Precompiled wheel not found. Building from source...\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m copying causal_conv1d/__init__.py -> build/lib.linux-x86_64-cpython-311/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m copying causal_conv1d/causal_conv1d_interface.py -> build/lib.linux-x86_64-cpython-311/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m /cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "  \u001b[31m   \u001b[0m /cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no g++ version bounds defined for CUDA version 12.5\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
      "  \u001b[31m   \u001b[0m building 'causal_conv1d_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-install-oxrjihye/causal-conv1d_bd3206d62e3647c5998372d9ec85c35c/build/temp.linux-x86_64-cpython-311/csrc\n",
      "  \u001b[31m   \u001b[0m Emitting ninja build file /tmp/pip-install-oxrjihye/causal-conv1d_bd3206d62e3647c5998372d9ec85c35c/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
      "  \u001b[31m   \u001b[0m Compiling objects...\n",
      "  \u001b[31m   \u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  \u001b[31m   \u001b[0m ninja: error: '/tmp/pip-install-oxrjihye/causal-conv1d_bd3206d62e3647c5998372d9ec85c35c/csrc/causal_conv1d.cpp', needed by '/tmp/pip-install-oxrjihye/causal-conv1d_bd3206d62e3647c5998372d9ec85c35c/build/temp.linux-x86_64-cpython-311/csrc/causal_conv1d.o', missing and no known rule to make it\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-oxrjihye/causal-conv1d_bd3206d62e3647c5998372d9ec85c35c/setup.py\", line 207, in run\n",
      "  \u001b[31m   \u001b[0m     urllib.request.urlretrieve(wheel_url, wheel_filename)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 241, in urlretrieve\n",
      "  \u001b[31m   \u001b[0m     with contextlib.closing(urlopen(url, data)) as fp:\n",
      "  \u001b[31m   \u001b[0m                             ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 216, in urlopen\n",
      "  \u001b[31m   \u001b[0m     return opener.open(url, data, timeout)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 525, in open\n",
      "  \u001b[31m   \u001b[0m     response = meth(req, response)\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 634, in http_response\n",
      "  \u001b[31m   \u001b[0m     response = self.parent.error(\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 563, in error\n",
      "  \u001b[31m   \u001b[0m     return self._call_chain(*args)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 496, in _call_chain\n",
      "  \u001b[31m   \u001b[0m     result = func(*args)\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 643, in http_error_default\n",
      "  \u001b[31m   \u001b[0m     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "  \u001b[31m   \u001b[0m urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m During handling of the above exception, another exception occurred:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2105, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     subprocess.run(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/subprocess.py\", line 571, in run\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-oxrjihye/causal-conv1d_bd3206d62e3647c5998372d9ec85c35c/setup.py\", line 227, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/__init__.py\", line 117, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 183, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-oxrjihye/causal-conv1d_bd3206d62e3647c5998372d9ec85c35c/setup.py\", line 224, in run\n",
      "  \u001b[31m   \u001b[0m     super().run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/wheel/_bdist_wheel.py\", line 378, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 98, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 866, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     build_ext.build_extensions(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 476, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 502, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 263, in build_extension\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_extension(self, ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 557, in build_extension\n",
      "  \u001b[31m   \u001b[0m     objects = self.compiler.compile(\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 679, in unix_wrap_ninja_compile\n",
      "  \u001b[31m   \u001b[0m     _write_ninja_file_and_compile_objects(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1785, in _write_ninja_file_and_compile_objects\n",
      "  \u001b[31m   \u001b[0m     _run_ninja_build(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2121, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(message) from e\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Error compiling objects for extension\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for causal-conv1d\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for causal-conv1d\n",
      "Failed to build mamba_ssm causal-conv1d\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (mamba_ssm, causal-conv1d)\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightly mambavision transformers mamba_ssm timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "# Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hugging Face transformers to load the MambaVision model\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "ZIP_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14.zip'\n",
    "EXTRACTED_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14-extracted'\n",
    "\n",
    "# Define parameters\n",
    "SEED = 42\n",
    "\n",
    "# Parameteres SimCLR\n",
    "LEARNING_RATE_SIMCLR = 1e-4\n",
    "SIMCLR_EPOCHS = 10\n",
    "BATCH_SIZE_SIMCLR = 32\n",
    "TEMPERATURE_SIMCLR = 0.5\n",
    "\n",
    "# Parameteres fine tuning\n",
    "LEARNING_RATE = 1e-4\n",
    "TEST_SIZE = 0.2\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Seed for Reproducibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    \"\"\"\n",
    "    Sets the seed to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Apply the seed\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(zip_path, extracted_path):\n",
    "    \"\"\"\n",
    "    Extracts the ZIP file of the dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(extracted_path, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    print(f\"Data extracted to {extracted_path}\")\n",
    "\n",
    "# Uncomment the line below to extract data (if not already extracted)\n",
    "# extract_data(ZIP_PATH, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Disease Labels\n",
    "disease_labels = [\n",
    "    'Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n",
    "    'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'\n",
    "]\n",
    "\n",
    "def load_labels(csv_path, image_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the labels from the CSV file.\n",
    "    Maps each image to its corresponding file path and binary labels for each disease.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the CSV file containing labels\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Create binary columns for each disease label\n",
    "    for disease in disease_labels:\n",
    "        labels_df[disease] = labels_df['Finding Labels'].str.contains(disease).astype(int)\n",
    "\n",
    "    # Create a binary column for 'No Finding'\n",
    "    labels_df['No_Finding'] = labels_df['Finding Labels'].apply(lambda x: 1 if 'No Finding' in x else 0)\n",
    "\n",
    "    # Map image filenames to their full paths\n",
    "    image_paths = glob(os.path.join(image_path, '**', 'images', '*.png'), recursive=True)\n",
    "    img_path_dict = {os.path.basename(path): path for path in image_paths}\n",
    "\n",
    "    # Add the full image path to the dataframe\n",
    "    labels_df['Path'] = labels_df['Image Index'].map(img_path_dict)\n",
    "    return labels_df\n",
    "\n",
    "# Path to the labels CSV file\n",
    "labels_csv_path = os.path.join(EXTRACTED_PATH, 'Data_Entry_2017.csv')\n",
    "\n",
    "# Load and preprocess the labels\n",
    "labels_df = load_labels(labels_csv_path, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 89826\n",
      "Test size: 22294\n"
     ]
    }
   ],
   "source": [
    "# Split patients into training/validation and test sets\n",
    "unique_patients = labels_df['Patient ID'].unique()\n",
    "train_val_patients, test_patients = train_test_split(\n",
    "    unique_patients, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Create training/validation and test dataframes\n",
    "train_val_df = labels_df[labels_df['Patient ID'].isin(train_val_patients)].reset_index(drop=True)\n",
    "test_df = labels_df[labels_df['Patient ID'].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "# Verify Split Sizes\n",
    "print(f\"Train size: {train_val_df.shape[0]}\")\n",
    "print(f\"Test size: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Dataset for Chest X-ray images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Chest X-ray images.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path and labels\n",
    "        img_path = self.df.iloc[idx]['Path']\n",
    "        image = plt.imread(img_path)\n",
    "        label = self.df.iloc[idx][disease_labels].values.astype(np.float32)\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Data Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training data\n",
    "train_transforms = transforms.Compose([\n",
    "\n",
    "    # Convert image to PIL format\n",
    "    transforms.ToPILImage(),\n",
    "\n",
    "    # Convert to grayscale and change to 3 channels\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "\n",
    "    # Resize the image to 224x224\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # Apply random horizontal flip to augment the data\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "\n",
    "    # Randomly rotate the image within a range of ±10 degrees\n",
    "    transforms.RandomRotation(10),\n",
    "\n",
    "    # Convert the image to a PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalize using ImageNet mean and std\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define transformations for test data\n",
    "test_transforms = transforms.Compose([\n",
    "\n",
    "    # Convert image to PIL format for further transformations\n",
    "    transforms.ToPILImage(),\n",
    "\n",
    "    # Convert to grayscale and change to 3 channels\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "\n",
    "    # Resize the image to 224x224\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # Convert the image to a PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalize using ImageNet mean and std\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChestXrayDataset(train_val_df, transform=train_transforms)\n",
    "test_dataset = ChestXrayDataset(test_df, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into Training and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int((1-TEST_SIZE) * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create DataLoaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 7: Build the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "966.42s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mamba_ssm in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (2.2.4)\n",
      "Requirement already satisfied: timm in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (1.0.14)\n",
      "Requirement already satisfied: torch in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from mamba_ssm) (2.4.1)\n",
      "Requirement already satisfied: ninja in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (1.11.1.3)\n",
      "Requirement already satisfied: einops in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (0.8.0)\n",
      "Requirement already satisfied: transformers in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (4.48.3)\n",
      "Requirement already satisfied: packaging in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (24.2)\n",
      "Requirement already satisfied: setuptools>=61.0.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (75.1.0)\n",
      "Requirement already satisfied: torchvision in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from timm) (0.19.1)\n",
      "Requirement already satisfied: pyyaml in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (0.26.2)\n",
      "Requirement already satisfied: safetensors in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface_hub->timm) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface_hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: requests in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from huggingface_hub->timm) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (1.13.2)\n",
      "Requirement already satisfied: networkx in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba_ssm) (12.6.68)\n",
      "Requirement already satisfied: numpy in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from torchvision->timm) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from transformers->mamba_ssm) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from transformers->mamba_ssm) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from jinja2->torch->mamba_ssm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from sympy->torch->mamba_ssm) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mamba_ssm timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the Multi-Label Classifier Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label Classification Model using Swin Transformer as the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=len(disease_labels)):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        # Load pre-trained Swin Transformer model\n",
    "        self.base_model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.base_model.classifier = nn.Linear(self.base_model.classifier.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Initialize the Model\n",
    "model = MultiLabelClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move Model to GPU if Available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 3.12 MiB is free. Including non-PyTorch memory, this process has 15.88 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 3.12 MiB is free. Including non-PyTorch memory, this process has 15.88 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Loss Function and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 8: Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/562 [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Including non-PyTorch memory, this process has 15.87 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 160.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_losses, val_losses, val_aucs\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Train the model for the desired number of epochs\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m train_losses, val_losses, val_aucs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 16\u001b[0m, in \u001b[0;36mMultiLabelClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchvision/models/densenet.py:213\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 213\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(features, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    215\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(out, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchvision/models/densenet.py:122\u001b[0m, in \u001b[0;36m_DenseBlock.forward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m    120\u001b[0m features \u001b[38;5;241m=\u001b[39m [init_features]\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 122\u001b[0m     new_features \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(new_features)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchvision/models/densenet.py:88\u001b[0m, in \u001b[0;36m_DenseLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     86\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_checkpoint_bottleneck(prev_features)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m new_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(bottleneck_output)))\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchvision/models/densenet.py:48\u001b[0m, in \u001b[0;36m_DenseLayer.bn_function\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbn_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 48\u001b[0m     concated_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(concated_features)))  \u001b[38;5;66;03m# noqa: T484\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bottleneck_output\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Including non-PyTorch memory, this process has 15.87 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 160.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates it on the validation set after each epoch.\n",
    "    \"\"\"\n",
    "    # List to store loss and AUC\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_aucs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        # Progress bar\n",
    "        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Disable gradients for evaluation\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Apply sigmoid\n",
    "                preds = torch.sigmoid(outputs).cpu()\n",
    "\n",
    "                # Store predictions and true labels\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels.cpu())\n",
    "                \n",
    "                progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Concatenate all batches\n",
    "        preds = torch.cat(all_preds)\n",
    "        labels = torch.cat(all_labels)\n",
    "    \n",
    "        roc_auc_per_label = []\n",
    "\n",
    "        # Compute metrics for each label\n",
    "        for i, label in enumerate(disease_labels):\n",
    "\n",
    "            # Calculate ROC AUC\n",
    "            roc_auc = roc_auc_score(labels[:, i].numpy(), preds[:, i].numpy())\n",
    "\n",
    "            roc_auc_per_label.append(roc_auc)\n",
    "\n",
    "        # Calculate and add overall metrics\n",
    "        mean_auc = np.mean(roc_auc_per_label)\n",
    "        val_aucs.append(mean_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {train_loss:.4f} | \"\n",
    "              f\"Validation Loss: {val_loss:.4f} | Mean Validation AUC: {mean_auc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, val_aucs\n",
    "\n",
    "# Train the model for the desired number of epochs\n",
    "train_losses, val_losses, val_aucs = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
