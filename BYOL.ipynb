{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "# Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.transforms.byol_transform import BYOLTransform, BYOLView1Transform, BYOLView2Transform\n",
    "from lightly.models.modules import BYOLProjectionHead, BYOLPredictionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.utils.scheduler import cosine_schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "ZIP_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14.zip'\n",
    "EXTRACTED_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14-extracted'\n",
    "\n",
    "# Define parameters\n",
    "SEED = 42\n",
    "\n",
    "# Parameteres BYOL\n",
    "LEARNING_RATE_BYOL = 0.06\n",
    "BYOL_EPOCHS = 1\n",
    "BATCH_SIZE_BYOL = 32\n",
    "TEMPERATURE_BYOL = 0.5\n",
    "\n",
    "# Parameteres fine tuning\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Seed for Reproducibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    \"\"\"\n",
    "    Sets the seed to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Apply the seed\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(zip_path, extracted_path):\n",
    "    \"\"\"\n",
    "    Extracts the ZIP file of the dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(extracted_path, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    print(f\"Data extracted to {extracted_path}\")\n",
    "\n",
    "# Uncomment the line below to extract data (if not already extracted)\n",
    "# extract_data(ZIP_PATH, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Disease Labels\n",
    "disease_labels = [\n",
    "    'Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n",
    "    'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'\n",
    "]\n",
    "\n",
    "def load_labels(csv_path, image_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the labels from the CSV file.\n",
    "    Maps each image to its corresponding file path and binary labels for each disease.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the CSV file containing labels\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Create binary columns for each disease label\n",
    "    for disease in disease_labels:\n",
    "        labels_df[disease] = labels_df['Finding Labels'].str.contains(disease).astype(int)\n",
    "\n",
    "    # Create a binary column for 'No Finding'\n",
    "    labels_df['No_Finding'] = labels_df['Finding Labels'].apply(lambda x: 1 if 'No Finding' in x else 0)\n",
    "\n",
    "    # Map image filenames to their full paths\n",
    "    image_paths = glob(os.path.join(image_path, '**', 'images', '*.png'), recursive=True)\n",
    "    img_path_dict = {os.path.basename(path): path for path in image_paths}\n",
    "\n",
    "    # Add the full image path to the dataframe\n",
    "    labels_df['Path'] = labels_df['Image Index'].map(img_path_dict)\n",
    "    return labels_df\n",
    "\n",
    "# Path to the labels CSV file\n",
    "labels_csv_path = os.path.join(EXTRACTED_PATH, 'Data_Entry_2017.csv')\n",
    "\n",
    "# Load and preprocess the labels\n",
    "labels_df = load_labels(labels_csv_path, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 89826\n",
      "Test size: 22294\n"
     ]
    }
   ],
   "source": [
    "# Split patients into training/validation and test sets\n",
    "unique_patients = labels_df['Patient ID'].unique()\n",
    "train_val_patients, test_patients = train_test_split(\n",
    "    unique_patients, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Create training/validation and test dataframes\n",
    "train_val_df = labels_df[labels_df['Patient ID'].isin(train_val_patients)].reset_index(drop=True)\n",
    "test_df = labels_df[labels_df['Patient ID'].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "# Verify Split Sizes\n",
    "print(f\"Train size: {train_val_df.shape[0]}\")\n",
    "print(f\"Test size: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Pre-train using BYOL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the BYOL transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "byol_transform = BYOLTransform(\n",
    "    view_1_transform=BYOLView1Transform(input_size=224, gaussian_blur=0.0),\n",
    "    view_2_transform=BYOLView2Transform(input_size=224, gaussian_blur=0.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the BYOL model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # The online network\n",
    "        self.backbone = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        self.projection_head = BYOLProjectionHead(512, 1024, 256)\n",
    "        self.prediction_head = BYOLPredictionHead(256, 1024, 256)\n",
    "\n",
    "        # The target network\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        # Freeze the teacher network. Only update using EMA\n",
    "        deactivate_requires_grad(self.backbone_momentum)\n",
    "        deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "    def forward_student(self, x):\n",
    "        # Forward pass student network\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        return p\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        # Forward pass teacher network\n",
    "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y)\n",
    "        return z.detach()\n",
    "\n",
    "# Initalize BYOL model\n",
    "byol_model = BYOL().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create BYOL Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOLDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for BYOL.\n",
    "    Returns two augmented versions of each image.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path\n",
    "        img_path = self.df.iloc[idx]['Path']\n",
    "\n",
    "        # Load image using PIL and convert to RGB\n",
    "        image = plt.imread(img_path)\n",
    "\n",
    "        # Apply the BYOL transform to generate two augmented views\n",
    "        view1, view2 = self.transform(image)\n",
    "\n",
    "        return view1, view2\n",
    "\n",
    "# Recreate SimCLR Dataset with the updated __getitem__ method\n",
    "byol_dataset = BYOLDataset(train_val_df, transform=byol_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize BYOL DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_loader = DataLoader(\n",
    "    byol_dataset,\n",
    "    batch_size=BATCH_SIZE_BYOL,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Optimizer and Loss for BYOL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NegativeCosineSimilarity()\n",
    "optimizer = torch.optim.SGD(byol_model.parameters(), lr=LEARNING_RATE_BYOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train BYOL Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_4047407/788149760.py\", line 21, in __getitem__\n    view1, view2 = self.transform(image)\n                   ^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/lightly/transforms/multi_view_transform.py\", line 33, in __call__\n    return [transform(image) for transform in self.transforms]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/lightly/transforms/multi_view_transform.py\", line 33, in <listcomp>\n    return [transform(image) for transform in self.transforms]\n            ^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/lightly/transforms/byol_transform.py\", line 70, in __call__\n    transformed: Tensor = self.transform(image)\n                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 972, in forward\n    i, j, h, w = self.get_params(img, self.scale, self.ratio)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 933, in get_params\n    _, height, width = F.get_dimensions(img)\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/functional.py\", line 80, in get_dimensions\n    return F_pil.get_dimensions(img)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/_functional_pil.py\", line 31, in get_dimensions\n    raise TypeError(f\"Unexpected type {type(img)}\")\nTypeError: Unexpected type <class 'numpy.ndarray'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Compute momentum coefficient using a cosine schedule.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m momentum_val \u001b[38;5;241m=\u001b[39m cosine_schedule(epoch, BYOL_EPOCHS, \u001b[38;5;241m0.996\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msimclr_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Each batch returns a tuple of two views: x0 and x1.\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Update the momentum networks (EMA update).\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_4047407/788149760.py\", line 21, in __getitem__\n    view1, view2 = self.transform(image)\n                   ^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/lightly/transforms/multi_view_transform.py\", line 33, in __call__\n    return [transform(image) for transform in self.transforms]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/lightly/transforms/multi_view_transform.py\", line 33, in <listcomp>\n    return [transform(image) for transform in self.transforms]\n            ^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/lightly/transforms/byol_transform.py\", line 70, in __call__\n    transformed: Tensor = self.transform(image)\n                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 972, in forward\n    i, j, h, w = self.get_params(img, self.scale, self.ratio)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 933, in get_params\n    _, height, width = F.get_dimensions(img)\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/functional.py\", line 80, in get_dimensions\n    return F_pil.get_dimensions(img)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torchvision/transforms/_functional_pil.py\", line 31, in get_dimensions\n    raise TypeError(f\"Unexpected type {type(img)}\")\nTypeError: Unexpected type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(BYOL_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    momentum_val = cosine_schedule(epoch, BYOL_EPOCHS, 0.996, 1)\n",
    "    for batch in simclr_loader:\n",
    "        x0, x1 = batch\n",
    "        update_momentum(model.backbone, model.backbone_momentum, m=momentum_val)\n",
    "        update_momentum(model.projection_head, model.projection_head_momentum, m=momentum_val)\n",
    "\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        p0 = model(x0)\n",
    "        z0 = model.forward_momentum(x0)\n",
    "        p1 = model(x1)\n",
    "        z1 = model.forward_momentum(x1)\n",
    "\n",
    "        loss = 0.5 * (criterion(p0, z1) + criterion(p1, z0))\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch: {epoch:02}, Loss: {avg_loss:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
