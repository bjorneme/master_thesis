{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "# Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightly.loss import NTXentLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "ZIP_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14.zip'\n",
    "EXTRACTED_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14-extracted'\n",
    "\n",
    "# Define parameters\n",
    "SEED = 42\n",
    "\n",
    "# Parameteres SimCLR\n",
    "LEARNING_RATE_SIMCLR = 1e-4\n",
    "SIMCLR_EPOCHS = 10\n",
    "BATCH_SIZE_SIMCLR = 32\n",
    "TEMPERATURE_SIMCLR = 0.5\n",
    "\n",
    "# Parameteres fine tuning\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Seed for Reproducibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    \"\"\"\n",
    "    Sets the seed to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Apply the seed\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(zip_path, extracted_path):\n",
    "    \"\"\"\n",
    "    Extracts the ZIP file of the dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(extracted_path, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    print(f\"Data extracted to {extracted_path}\")\n",
    "\n",
    "# Uncomment the line below to extract data (if not already extracted)\n",
    "# extract_data(ZIP_PATH, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Disease Labels\n",
    "disease_labels = [\n",
    "    'Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n",
    "    'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'\n",
    "]\n",
    "\n",
    "def load_labels(csv_path, image_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the labels from the CSV file.\n",
    "    Maps each image to its corresponding file path and binary labels for each disease.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the CSV file containing labels\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Create binary columns for each disease label\n",
    "    for disease in disease_labels:\n",
    "        labels_df[disease] = labels_df['Finding Labels'].str.contains(disease).astype(int)\n",
    "\n",
    "    # Create a binary column for 'No Finding'\n",
    "    labels_df['No_Finding'] = labels_df['Finding Labels'].apply(lambda x: 1 if 'No Finding' in x else 0)\n",
    "\n",
    "    # Map image filenames to their full paths\n",
    "    image_paths = glob(os.path.join(image_path, '**', 'images', '*.png'), recursive=True)\n",
    "    img_path_dict = {os.path.basename(path): path for path in image_paths}\n",
    "\n",
    "    # Add the full image path to the dataframe\n",
    "    labels_df['Path'] = labels_df['Image Index'].map(img_path_dict)\n",
    "    return labels_df\n",
    "\n",
    "# Path to the labels CSV file\n",
    "labels_csv_path = os.path.join(EXTRACTED_PATH, 'Data_Entry_2017.csv')\n",
    "\n",
    "# Load and preprocess the labels\n",
    "labels_df = load_labels(labels_csv_path, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 89826\n",
      "Test size: 22294\n"
     ]
    }
   ],
   "source": [
    "# Split patients into training/validation and test sets\n",
    "unique_patients = labels_df['Patient ID'].unique()\n",
    "train_val_patients, test_patients = train_test_split(\n",
    "    unique_patients, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Create training/validation and test dataframes\n",
    "train_val_df = labels_df[labels_df['Patient ID'].isin(train_val_patients)].reset_index(drop=True)\n",
    "test_df = labels_df[labels_df['Patient ID'].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "# Verify Split Sizes\n",
    "print(f\"Train size: {train_val_df.shape[0]}\")\n",
    "print(f\"Test size: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Pre-train using SimCLR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the transformation for SimCLR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRTransform:\n",
    "    \"\"\"\n",
    "    Generates two augmented versions of the same image.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_transform):\n",
    "        self.base_transform = base_transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.base_transform(x), self.base_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base transformation for SimCLR\n",
    "simclr_transform = transforms.Compose([\n",
    "\n",
    "    # Convert image to PIL format\n",
    "    transforms.ToPILImage(),\n",
    "\n",
    "    # Convert to grayscale and change to 3 channels\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "\n",
    "    # Resize the image to 224x224\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # Apply random horizontal flip to augment the data\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "\n",
    "    # Randomly rotate the image within a range of Â±10 degrees\n",
    "    transforms.RandomRotation(10),\n",
    "\n",
    "    # Convert the image to a PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalize using ImageNet mean and std\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize SimCLR Transform\n",
    "simclr_data_transform = SimCLRTransform(simclr_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create SimCLR Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for SimCLR.\n",
    "    Returns two augmented versions of each image.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path\n",
    "        img_path = self.df.iloc[idx]['Path']\n",
    "        \n",
    "        # Load image using PIL and convert to RGB\n",
    "        image = plt.imread(img_path)\n",
    "\n",
    "        # Apply SimCLR transformations to get two views\n",
    "        if self.transform:\n",
    "            img1, img2 = self.transform(image)\n",
    "        else:\n",
    "            img1, img2 = image, image\n",
    "\n",
    "        return img1, img2\n",
    "\n",
    "# Recreate SimCLR Dataset with the updated __getitem__ method\n",
    "simclr_dataset = SimCLRDataset(train_val_df, transform=simclr_data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize SimCLR DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "simclr_loader = DataLoader(\n",
    "    simclr_dataset,\n",
    "    batch_size=BATCH_SIZE_SIMCLR,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define SimCLR Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRModel(nn.Module):\n",
    "    \"\"\"\n",
    "    SimCLR Model with base encoder and projection head.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=len(disease_labels)):\n",
    "        super(SimCLRModel, self).__init__()\n",
    "        \n",
    "        # Load pre-trained Swin Transformer model\n",
    "        self.base_model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.base_model.classifier = nn.Linear(self.base_model.classifier.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Initialize SimCLR Model\n",
    "simclr_model = SimCLRModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Optimizer and NT-Xent Loss for SimCLR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_simclr = optim.Adam(simclr_model.parameters(), lr=LEARNING_RATE_SIMCLR)\n",
    "criterion_simclr = NTXentLoss(temperature=0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Train SimCLR Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 1/10:   0%|          | 0/2807 [00:00<?, ?it/s]/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "SimCLR Epoch 1/10:   0%|          | 0/2807 [00:14<?, ?it/s, Loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 2/10:   0%|          | 0/2807 [00:13<?, ?it/s, Loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] - Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 3/10:   0%|          | 0/2807 [00:13<?, ?it/s, Loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] - Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 4/10:   0%|          | 0/2807 [00:13<?, ?it/s, Loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] - Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 5/10:   0%|          | 0/2807 [00:13<?, ?it/s, Loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] - Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 6/10:   0%|          | 0/2807 [00:14<?, ?it/s, Loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] - Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 7/10:   0%|          | 0/2807 [00:13<?, ?it/s, Loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] - Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 8/10:   0%|          | 0/2807 [00:13<?, ?it/s, Loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] - Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 9/10:   0%|          | 0/2807 [00:13<?, ?it/s, Loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] - Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimCLR Epoch 10/10:   0%|          | 0/2807 [00:13<?, ?it/s, Loss=2.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] - Average Loss: 0.0009\n",
      "SimCLR Training Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_simclr(model, data_loader, criterion, optimizer, epochs=100):\n",
    "    \"\"\"\n",
    "    Trains the SimCLR model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(data_loader, desc=f\"SimCLR Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            img1, img2 = batch\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            z_i = model(img1)\n",
    "            z_j = model(img2)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(z_i, z_j)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"SimCLR Training Completed.\")\n",
    "\n",
    "# Start SimCLR pretraining\n",
    "train_simclr(simclr_model, simclr_loader, criterion_simclr, optimizer_simclr, epochs=SIMCLR_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save pre-trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimCLR encoder saved.\n"
     ]
    }
   ],
   "source": [
    "# After SimCLR Training is Completed\n",
    "torch.save(simclr_model.state_dict(), 'simclr_model.pth')\n",
    "print(\"SimCLR encoder saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
