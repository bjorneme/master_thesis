{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightly in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (1.5.14)\n",
      "Collecting mambavision\n",
      "  Using cached mambavision-1.0.9-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (4.48.3)\n",
      "Requirement already satisfied: mamba_ssm in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (2.2.4)\n",
      "Requirement already satisfied: timm in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (1.0.14)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2024.8.30)\n",
      "Requirement already satisfied: hydra-core>=1.0.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (1.3.2)\n",
      "Requirement already satisfied: lightly-utils~=0.0.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (0.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly) (2.9.0.post0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2.32.3)\n",
      "Requirement already satisfied: six>=1.10 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly) (1.16.0)\n",
      "Requirement already satisfied: tqdm>=4.44 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (4.67.0)\n",
      "Requirement already satisfied: torch in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly) (0.19.1)\n",
      "Requirement already satisfied: pydantic>=1.10.5 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2.9.2)\n",
      "Requirement already satisfied: pytorch-lightning>=1.0.4 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (1.9.5)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (2.2.3)\n",
      "Requirement already satisfied: aenum>=3.1.11 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightly) (3.1.15)\n",
      "Collecting timm\n",
      "  Using cached timm-0.9.0-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting mamba_ssm\n",
      "  Using cached mamba_ssm-1.0.1.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting causal-conv1d==1.0.2 (from mambavision)\n",
      "  Using cached causal_conv1d-1.0.2.tar.gz (6.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mambavision) (0.8.0)\n",
      "Requirement already satisfied: packaging in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (24.2)\n",
      "Requirement already satisfied: ninja in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (1.11.1.3)\n",
      "Requirement already satisfied: triton in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from mamba_ssm) (3.0.0)\n",
      "Requirement already satisfied: pyyaml in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (0.26.2)\n",
      "Requirement already satisfied: safetensors in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (0.4.5)\n",
      "Collecting buildtools (from causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached buildtools-1.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface-hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface-hub->timm) (4.12.2)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from hydra-core>=1.0.0->lightly) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from hydra-core>=1.0.0->lightly) (4.9.3)\n",
      "Requirement already satisfied: Pillow in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from lightly-utils~=0.0.0->lightly) (10.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from pydantic>=1.10.5->lightly) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from pydantic>=1.10.5->lightly) (2.23.4)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from pytorch-lightning>=1.0.4->lightly) (1.6.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from pytorch-lightning>=1.0.4->lightly) (0.11.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests>=2.23.0->lightly) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests>=2.23.0->lightly) (3.10)\n",
      "Requirement already satisfied: sympy in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (1.13.2)\n",
      "Requirement already satisfied: networkx in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (3.3)\n",
      "Requirement already satisfied: jinja2 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->lightly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->lightly) (12.6.68)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (3.11.7)\n",
      "Requirement already satisfied: setuptools in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from lightning-utilities>=0.6.0.post0->pytorch-lightning>=1.0.4->lightly) (75.1.0)\n",
      "Collecting sqlalchemy (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached SQLAlchemy-2.0.38-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting argparse (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting twisted (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting simplejson (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting furl (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached furl-2.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting docopt (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting redo (from buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached redo-3.0.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from jinja2->torch->lightly) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from sympy->torch->lightly) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.0.4->lightly) (1.18.0)\n",
      "Collecting orderedmultidict>=1.0.1 (from furl->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting automat>=24.8.0 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting constantly>=15.1 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=24.7.0 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting zope-interface>=5 (from twisted->buildtools->causal-conv1d==1.0.2->mambavision)\n",
      "  Using cached zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Using cached mambavision-1.0.9-py3-none-any.whl (56 kB)\n",
      "Using cached timm-0.9.0-py3-none-any.whl (2.2 MB)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Using cached furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Using cached redo-3.0.0-py2.py3-none-any.whl (14 kB)\n",
      "Using cached simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Using cached SQLAlchemy-2.0.38-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
      "Using cached Automat-24.8.1-py3-none-any.whl (42 kB)\n",
      "Using cached constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
      "Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Using cached incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Using cached orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
      "Building wheels for collected packages: mamba_ssm, causal-conv1d\n",
      "  Building wheel for mamba_ssm (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[135 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.4.1+cu121\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m Guessing wheel URL:  https://github.com/state-spaces/mamba/releases/download/v1.0.1/mamba_ssm-1.0.1+cu122torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
      "  \u001b[31m   \u001b[0m Precompiled wheel not found. Building from source...\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/models\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/models/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/models\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/models/mixer_seq_simple.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/modules\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/modules/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/modules\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/modules/mamba_simple.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/ops\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/selective_scan_interface.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/utils\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/utils\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/utils/generation.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/utils\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/utils/hf.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/mamba_ssm/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/triton/__init__.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/triton/layernorm.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying mamba_ssm/ops/triton/selective_state_update.py -> build/lib.linux-x86_64-cpython-311/mamba_ssm/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m /cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "  \u001b[31m   \u001b[0m /cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no g++ version bounds defined for CUDA version 12.5\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
      "  \u001b[31m   \u001b[0m building 'selective_scan_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-install-i8d1emke/mamba-ssm_1d8c4655b68f4a4daa0b2283d43e07ef/build/temp.linux-x86_64-cpython-311/csrc/selective_scan\n",
      "  \u001b[31m   \u001b[0m Emitting ninja build file /tmp/pip-install-i8d1emke/mamba-ssm_1d8c4655b68f4a4daa0b2283d43e07ef/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
      "  \u001b[31m   \u001b[0m Compiling objects...\n",
      "  \u001b[31m   \u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  \u001b[31m   \u001b[0m ninja: error: '/tmp/pip-install-i8d1emke/mamba-ssm_1d8c4655b68f4a4daa0b2283d43e07ef/csrc/selective_scan/selective_scan.cpp', needed by '/tmp/pip-install-i8d1emke/mamba-ssm_1d8c4655b68f4a4daa0b2283d43e07ef/build/temp.linux-x86_64-cpython-311/csrc/selective_scan/selective_scan.o', missing and no known rule to make it\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-i8d1emke/mamba-ssm_1d8c4655b68f4a4daa0b2283d43e07ef/setup.py\", line 213, in run\n",
      "  \u001b[31m   \u001b[0m     urllib.request.urlretrieve(wheel_url, wheel_filename)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 241, in urlretrieve\n",
      "  \u001b[31m   \u001b[0m     with contextlib.closing(urlopen(url, data)) as fp:\n",
      "  \u001b[31m   \u001b[0m                             ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 216, in urlopen\n",
      "  \u001b[31m   \u001b[0m     return opener.open(url, data, timeout)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 525, in open\n",
      "  \u001b[31m   \u001b[0m     response = meth(req, response)\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 634, in http_response\n",
      "  \u001b[31m   \u001b[0m     response = self.parent.error(\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 563, in error\n",
      "  \u001b[31m   \u001b[0m     return self._call_chain(*args)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 496, in _call_chain\n",
      "  \u001b[31m   \u001b[0m     result = func(*args)\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 643, in http_error_default\n",
      "  \u001b[31m   \u001b[0m     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "  \u001b[31m   \u001b[0m urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m During handling of the above exception, another exception occurred:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2105, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     subprocess.run(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/subprocess.py\", line 571, in run\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-i8d1emke/mamba-ssm_1d8c4655b68f4a4daa0b2283d43e07ef/setup.py\", line 233, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/__init__.py\", line 117, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 183, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-i8d1emke/mamba-ssm_1d8c4655b68f4a4daa0b2283d43e07ef/setup.py\", line 230, in run\n",
      "  \u001b[31m   \u001b[0m     super().run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/wheel/_bdist_wheel.py\", line 378, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 98, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 866, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     build_ext.build_extensions(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 476, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 502, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 263, in build_extension\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_extension(self, ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 557, in build_extension\n",
      "  \u001b[31m   \u001b[0m     objects = self.compiler.compile(\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 679, in unix_wrap_ninja_compile\n",
      "  \u001b[31m   \u001b[0m     _write_ninja_file_and_compile_objects(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1785, in _write_ninja_file_and_compile_objects\n",
      "  \u001b[31m   \u001b[0m     _run_ninja_build(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2121, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(message) from e\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Error compiling objects for extension\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for mamba_ssm\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for mamba_ssm\n",
      "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[119 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.4.1+cu121\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m Guessing wheel URL:  https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.0.2/causal_conv1d-1.0.2+cu122torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
      "  \u001b[31m   \u001b[0m Precompiled wheel not found. Building from source...\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m copying causal_conv1d/__init__.py -> build/lib.linux-x86_64-cpython-311/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m copying causal_conv1d/causal_conv1d_interface.py -> build/lib.linux-x86_64-cpython-311/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m /cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "  \u001b[31m   \u001b[0m /cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no g++ version bounds defined for CUDA version 12.5\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
      "  \u001b[31m   \u001b[0m building 'causal_conv1d_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-install-i8d1emke/causal-conv1d_2f84137cae3742498e983075ca050cac/build/temp.linux-x86_64-cpython-311/csrc\n",
      "  \u001b[31m   \u001b[0m Emitting ninja build file /tmp/pip-install-i8d1emke/causal-conv1d_2f84137cae3742498e983075ca050cac/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
      "  \u001b[31m   \u001b[0m Compiling objects...\n",
      "  \u001b[31m   \u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  \u001b[31m   \u001b[0m ninja: error: '/tmp/pip-install-i8d1emke/causal-conv1d_2f84137cae3742498e983075ca050cac/csrc/causal_conv1d.cpp', needed by '/tmp/pip-install-i8d1emke/causal-conv1d_2f84137cae3742498e983075ca050cac/build/temp.linux-x86_64-cpython-311/csrc/causal_conv1d.o', missing and no known rule to make it\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-i8d1emke/causal-conv1d_2f84137cae3742498e983075ca050cac/setup.py\", line 207, in run\n",
      "  \u001b[31m   \u001b[0m     urllib.request.urlretrieve(wheel_url, wheel_filename)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 241, in urlretrieve\n",
      "  \u001b[31m   \u001b[0m     with contextlib.closing(urlopen(url, data)) as fp:\n",
      "  \u001b[31m   \u001b[0m                             ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 216, in urlopen\n",
      "  \u001b[31m   \u001b[0m     return opener.open(url, data, timeout)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 525, in open\n",
      "  \u001b[31m   \u001b[0m     response = meth(req, response)\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 634, in http_response\n",
      "  \u001b[31m   \u001b[0m     response = self.parent.error(\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 563, in error\n",
      "  \u001b[31m   \u001b[0m     return self._call_chain(*args)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 496, in _call_chain\n",
      "  \u001b[31m   \u001b[0m     result = func(*args)\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/urllib/request.py\", line 643, in http_error_default\n",
      "  \u001b[31m   \u001b[0m     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "  \u001b[31m   \u001b[0m urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m During handling of the above exception, another exception occurred:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2105, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     subprocess.run(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/subprocess.py\", line 571, in run\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-i8d1emke/causal-conv1d_2f84137cae3742498e983075ca050cac/setup.py\", line 227, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/__init__.py\", line 117, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 183, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-i8d1emke/causal-conv1d_2f84137cae3742498e983075ca050cac/setup.py\", line 224, in run\n",
      "  \u001b[31m   \u001b[0m     super().run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/wheel/_bdist_wheel.py\", line 378, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 98, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 866, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     build_ext.build_extensions(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 476, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 502, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 263, in build_extension\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_extension(self, ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 557, in build_extension\n",
      "  \u001b[31m   \u001b[0m     objects = self.compiler.compile(\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 679, in unix_wrap_ninja_compile\n",
      "  \u001b[31m   \u001b[0m     _write_ninja_file_and_compile_objects(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1785, in _write_ninja_file_and_compile_objects\n",
      "  \u001b[31m   \u001b[0m     _run_ninja_build(\n",
      "  \u001b[31m   \u001b[0m   File \"/cluster/home/bjorneme/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2121, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(message) from e\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Error compiling objects for extension\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for causal-conv1d\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for causal-conv1d\n",
      "Failed to build mamba_ssm causal-conv1d\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (mamba_ssm, causal-conv1d)\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightly mambavision transformers mamba_ssm timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "# Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, f1_score\n",
    "\n",
    "# Hugging Face transformers to load the MambaVision model\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "ZIP_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14.zip'\n",
    "EXTRACTED_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14-extracted'\n",
    "\n",
    "# Define parameters\n",
    "SEED = 42\n",
    "\n",
    "# Parameteres SimCLR\n",
    "LEARNING_RATE_SIMCLR = 1e-4\n",
    "SIMCLR_EPOCHS = 10\n",
    "BATCH_SIZE_SIMCLR = 32\n",
    "TEMPERATURE_SIMCLR = 0.5\n",
    "\n",
    "# Parameteres fine tuning\n",
    "LEARNING_RATE = 1e-4\n",
    "TEST_SIZE = 0.2\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Seed for Reproducibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    \"\"\"\n",
    "    Sets the seed to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Apply the seed\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(zip_path, extracted_path):\n",
    "    \"\"\"\n",
    "    Extracts the ZIP file of the dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(extracted_path, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    print(f\"Data extracted to {extracted_path}\")\n",
    "\n",
    "# Uncomment the line below to extract data (if not already extracted)\n",
    "# extract_data(ZIP_PATH, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Disease Labels\n",
    "disease_labels = [\n",
    "    'Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n",
    "    'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'\n",
    "]\n",
    "\n",
    "def load_labels(csv_path, image_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the labels from the CSV file.\n",
    "    Maps each image to its corresponding file path and binary labels for each disease.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the CSV file containing labels\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Create binary columns for each disease label\n",
    "    for disease in disease_labels:\n",
    "        labels_df[disease] = labels_df['Finding Labels'].str.contains(disease).astype(int)\n",
    "\n",
    "    # Create a binary column for 'No Finding'\n",
    "    labels_df['No_Finding'] = labels_df['Finding Labels'].apply(lambda x: 1 if 'No Finding' in x else 0)\n",
    "\n",
    "    # Map image filenames to their full paths\n",
    "    image_paths = glob(os.path.join(image_path, '**', 'images', '*.png'), recursive=True)\n",
    "    img_path_dict = {os.path.basename(path): path for path in image_paths}\n",
    "\n",
    "    # Add the full image path to the dataframe\n",
    "    labels_df['Path'] = labels_df['Image Index'].map(img_path_dict)\n",
    "    return labels_df\n",
    "\n",
    "# Path to the labels CSV file\n",
    "labels_csv_path = os.path.join(EXTRACTED_PATH, 'Data_Entry_2017.csv')\n",
    "\n",
    "# Load and preprocess the labels\n",
    "labels_df = load_labels(labels_csv_path, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 89826\n",
      "Test size: 22294\n"
     ]
    }
   ],
   "source": [
    "# Split patients into training/validation and test sets\n",
    "unique_patients = labels_df['Patient ID'].unique()\n",
    "train_val_patients, test_patients = train_test_split(\n",
    "    unique_patients, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Create training/validation and test dataframes\n",
    "train_val_df = labels_df[labels_df['Patient ID'].isin(train_val_patients)].reset_index(drop=True)\n",
    "test_df = labels_df[labels_df['Patient ID'].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "# Verify Split Sizes\n",
    "print(f\"Train size: {train_val_df.shape[0]}\")\n",
    "print(f\"Test size: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Dataset for Chest X-ray images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Chest X-ray images.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path and labels\n",
    "        img_path = self.df.iloc[idx]['Path']\n",
    "        image = plt.imread(img_path)\n",
    "        label = self.df.iloc[idx][disease_labels].values.astype(np.float32)\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Data Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training data\n",
    "train_transforms = transforms.Compose([\n",
    "\n",
    "    # Convert image to PIL format\n",
    "    transforms.ToPILImage(),\n",
    "\n",
    "    # Convert to grayscale and change to 3 channels\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "\n",
    "    # Resize the image to 224x224\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # Apply random horizontal flip to augment the data\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "\n",
    "    # Randomly rotate the image within a range of ±10 degrees\n",
    "    transforms.RandomRotation(10),\n",
    "\n",
    "    # Convert the image to a PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalize using ImageNet mean and std\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define transformations for test data\n",
    "test_transforms = transforms.Compose([\n",
    "\n",
    "    # Convert image to PIL format for further transformations\n",
    "    transforms.ToPILImage(),\n",
    "\n",
    "    # Convert to grayscale and change to 3 channels\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "\n",
    "    # Resize the image to 224x224\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # Convert the image to a PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalize using ImageNet mean and std\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChestXrayDataset(train_val_df, transform=train_transforms)\n",
    "test_dataset = ChestXrayDataset(test_df, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into Training and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int((1-TEST_SIZE) * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create DataLoaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 7: Build the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mamba_ssm in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (2.2.4)\n",
      "Requirement already satisfied: timm in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (1.0.14)\n",
      "Requirement already satisfied: torch in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from mamba_ssm) (2.4.1)\n",
      "Requirement already satisfied: ninja in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (1.11.1.3)\n",
      "Requirement already satisfied: einops in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (0.8.0)\n",
      "Requirement already satisfied: transformers in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (4.48.3)\n",
      "Requirement already satisfied: packaging in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (24.2)\n",
      "Requirement already satisfied: setuptools>=61.0.0 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from mamba_ssm) (75.1.0)\n",
      "Requirement already satisfied: torchvision in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from timm) (0.19.1)\n",
      "Requirement already satisfied: pyyaml in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (0.26.2)\n",
      "Requirement already satisfied: safetensors in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface_hub->timm) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface_hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: requests in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from huggingface_hub->timm) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (1.13.2)\n",
      "Requirement already satisfied: networkx in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torch->mamba_ssm) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba_ssm) (12.6.68)\n",
      "Requirement already satisfied: numpy in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from torchvision->timm) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from transformers->mamba_ssm) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from transformers->mamba_ssm) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from jinja2->torch->mamba_ssm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cluster/home/bjorneme/.local/lib/python3.11/site-packages (from sympy->torch->mamba_ssm) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mamba_ssm timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the Multi-Label Classifier Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaVisionModel(\n",
      "  (model): MambaVision(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Identity()\n",
      "      (conv_down): Sequential(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(32, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(80, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (levels): ModuleList(\n",
      "      (0): MambaVisionLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): ConvBlock(\n",
      "            (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act1): GELU(approximate='tanh')\n",
      "            (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "        (downsample): Downsample(\n",
      "          (reduction): Sequential(\n",
      "            (0): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): MambaVisionLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): ConvBlock(\n",
      "            (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act1): GELU(approximate='tanh')\n",
      "            (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (drop_path): DropPath(drop_prob=0.011)\n",
      "          )\n",
      "          (1): ConvBlock(\n",
      "            (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act1): GELU(approximate='tanh')\n",
      "            (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (drop_path): DropPath(drop_prob=0.022)\n",
      "          )\n",
      "          (2): ConvBlock(\n",
      "            (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act1): GELU(approximate='tanh')\n",
      "            (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (drop_path): DropPath(drop_prob=0.033)\n",
      "          )\n",
      "        )\n",
      "        (downsample): Downsample(\n",
      "          (reduction): Sequential(\n",
      "            (0): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): MambaVisionLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): MambaVisionMixer(\n",
      "              (in_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (x_proj): Linear(in_features=160, out_features=36, bias=False)\n",
      "              (dt_proj): Linear(in_features=20, out_features=160, bias=True)\n",
      "              (out_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (conv1d_x): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "              (conv1d_z): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.044)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): MambaVisionMixer(\n",
      "              (in_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (x_proj): Linear(in_features=160, out_features=36, bias=False)\n",
      "              (dt_proj): Linear(in_features=20, out_features=160, bias=True)\n",
      "              (out_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (conv1d_x): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "              (conv1d_z): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.056)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): MambaVisionMixer(\n",
      "              (in_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (x_proj): Linear(in_features=160, out_features=36, bias=False)\n",
      "              (dt_proj): Linear(in_features=20, out_features=160, bias=True)\n",
      "              (out_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (conv1d_x): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "              (conv1d_z): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.067)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): MambaVisionMixer(\n",
      "              (in_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (x_proj): Linear(in_features=160, out_features=36, bias=False)\n",
      "              (dt_proj): Linear(in_features=20, out_features=160, bias=True)\n",
      "              (out_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (conv1d_x): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "              (conv1d_z): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.078)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): MambaVisionMixer(\n",
      "              (in_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (x_proj): Linear(in_features=160, out_features=36, bias=False)\n",
      "              (dt_proj): Linear(in_features=20, out_features=160, bias=True)\n",
      "              (out_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (conv1d_x): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "              (conv1d_z): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.089)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): MambaVisionMixer(\n",
      "              (in_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (x_proj): Linear(in_features=160, out_features=36, bias=False)\n",
      "              (dt_proj): Linear(in_features=20, out_features=160, bias=True)\n",
      "              (out_proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (conv1d_x): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "              (conv1d_z): Conv1d(160, 160, kernel_size=(3,), stride=(1,), groups=160, bias=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.100)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.111)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.122)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.133)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.144)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): Block(\n",
      "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.156)\n",
      "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): Downsample(\n",
      "          (reduction): Sequential(\n",
      "            (0): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): MambaVisionLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): MambaVisionMixer(\n",
      "              (in_proj): Linear(in_features=640, out_features=640, bias=False)\n",
      "              (x_proj): Linear(in_features=320, out_features=56, bias=False)\n",
      "              (dt_proj): Linear(in_features=40, out_features=320, bias=True)\n",
      "              (out_proj): Linear(in_features=640, out_features=640, bias=False)\n",
      "              (conv1d_x): Conv1d(320, 320, kernel_size=(3,), stride=(1,), groups=320, bias=False)\n",
      "              (conv1d_z): Conv1d(320, 320, kernel_size=(3,), stride=(1,), groups=320, bias=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.167)\n",
      "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): Block(\n",
      "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): MambaVisionMixer(\n",
      "              (in_proj): Linear(in_features=640, out_features=640, bias=False)\n",
      "              (x_proj): Linear(in_features=320, out_features=56, bias=False)\n",
      "              (dt_proj): Linear(in_features=40, out_features=320, bias=True)\n",
      "              (out_proj): Linear(in_features=640, out_features=640, bias=False)\n",
      "              (conv1d_x): Conv1d(320, 320, kernel_size=(3,), stride=(1,), groups=320, bias=False)\n",
      "              (conv1d_z): Conv1d(320, 320, kernel_size=(3,), stride=(1,), groups=320, bias=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.178)\n",
      "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Block(\n",
      "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): Attention(\n",
      "              (qkv): Linear(in_features=640, out_features=1920, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=640, out_features=640, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.189)\n",
      "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Block(\n",
      "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "            (mixer): Attention(\n",
      "              (qkv): Linear(in_features=640, out_features=1920, bias=True)\n",
      "              (q_norm): Identity()\n",
      "              (k_norm): Identity()\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=640, out_features=640, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.200)\n",
      "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (head): Linear(in_features=640, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label Classification Model using Swin Transformer as the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=len(disease_labels), model_name=\"nvidia/MambaVision-T2-1K\"):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        # Load pre-trained Vision Mamba model\n",
    "        self.base_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        print(self.base_model)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.classifier = nn.Linear(self.base_model.model.head.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool, _ = self.base_model(x)\n",
    "        logits = self.classifier(avg_pool)\n",
    "        return logits\n",
    "\n",
    "# Initialize the Model\n",
    "model = MultiLabelClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move Model to GPU if Available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Loss Function and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 8: Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 2246/2246 [12:05<00:00,  3.10it/s, Loss=0.116] \n",
      "Epoch 1/3: 100%|██████████| 562/562 [03:14<00:00,  2.88it/s, Loss=0.0942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Training Loss: 0.2294 | Validation Loss: 0.1527 | Mean Validation AUC: 0.7891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  19%|█▉        | 424/2246 [01:43<06:05,  4.99it/s, Loss=0.152] "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates it on the validation set after each epoch.\n",
    "    \"\"\"\n",
    "    # List to store loss and AUC\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_aucs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        # Progress bar\n",
    "        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Disable gradients for evaluation\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Apply sigmoid\n",
    "                preds = torch.sigmoid(outputs).cpu()\n",
    "\n",
    "                # Store predictions and true labels\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels.cpu())\n",
    "                \n",
    "                progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Concatenate all batches\n",
    "        preds = torch.cat(all_preds)\n",
    "        labels = torch.cat(all_labels)\n",
    "    \n",
    "        roc_auc_per_label = []\n",
    "\n",
    "        # Compute metrics for each label\n",
    "        for i, label in enumerate(disease_labels):\n",
    "\n",
    "            # Calculate ROC AUC\n",
    "            roc_auc = roc_auc_score(labels[:, i].numpy(), preds[:, i].numpy())\n",
    "\n",
    "            roc_auc_per_label.append(roc_auc)\n",
    "\n",
    "        # Calculate and add overall metrics\n",
    "        mean_auc = np.mean(roc_auc_per_label)\n",
    "        val_aucs.append(mean_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {train_loss:.4f} | \"\n",
    "              f\"Validation Loss: {val_loss:.4f} | Mean Validation AUC: {mean_auc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, val_aucs\n",
    "\n",
    "# Train the model for the desired number of epochs\n",
    "train_losses, val_losses, val_aucs = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 9: Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and returns predictions and true labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Progress bar\n",
    "    progress_bar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n",
    "\n",
    "    # Disable gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Apply sigmoid\n",
    "            preds = torch.sigmoid(outputs).cpu()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Concatenate all batches\n",
    "    preds = torch.cat(all_preds)\n",
    "    labels = torch.cat(all_labels)\n",
    "    return preds, labels\n",
    "\n",
    "# Get predictions and true labels\n",
    "preds, labels = evaluate_model(fine_tuned_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Metrics for Each Label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries for metrics\n",
    "accuracy_per_label = {}\n",
    "roc_auc_per_label = {}\n",
    "\n",
    "# Compute metrics for each label\n",
    "for i, label in enumerate(disease_labels):\n",
    "    \n",
    "    # Binarize predictions with threshold 0.5\n",
    "    binary_preds = preds[:, i] > 0.5\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(labels[:, i], binary_preds)\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(labels[:, i], preds[:, i])\n",
    "\n",
    "    # Store metrics\n",
    "    accuracy_per_label[label] = acc\n",
    "    roc_auc_per_label[label] = roc_auc\n",
    "\n",
    "# Create a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Accuracy': accuracy_per_label,\n",
    "    'ROC AUC': roc_auc_per_label\n",
    "})\n",
    "\n",
    "# Calculate and add overall metrics\n",
    "overall_roc_auc = metrics_df['ROC AUC'].mean()\n",
    "metrics_df.loc['Overall'] = metrics_df.mean()\n",
    "\n",
    "# Display metrics\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot ROC curves for each label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, label in enumerate(disease_labels):\n",
    "    fpr, tpr, _ = roc_curve(labels[:, i], preds[:, i])\n",
    "    plt.plot(fpr, tpr, label=f\"{label} (AUC = {roc_auc_per_label[label]:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Each Disease Label')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Training and Validation Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Validation AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_aucs, label='Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Validation ROC AUC Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
